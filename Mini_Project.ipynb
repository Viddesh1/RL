{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version:  3.11.2\n",
      "Numpy Version:  1.23.5\n",
      "Gym Version:  0.24.0\n",
      "Pygame Version:  2.3.0\n"
     ]
    }
   ],
   "source": [
    "# Importing the necessary libraries.\n",
    "\n",
    "import numpy as np # For fast numeric / linear algebra computation.\n",
    "import time        # For controling time of execution.  \n",
    "import pickle      # For storing updated Q-table.\n",
    "import gym         # For working with open AI frozen lake v1 environment and utilities.\n",
    "import pygame      # For rendering the game and gym dependencies.\n",
    "\n",
    "from platform import python_version\n",
    "print(\"Python Version: \", python_version())\n",
    "print(\"Numpy Version: \", np.__version__)\n",
    "print(\"Gym Version: \", gym.__version__)\n",
    "print(\"Pygame Version: \", pygame.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-Parameters\n",
    "\n",
    "total_episodes = 10000 # Total number of iterations or episodes of training.\n",
    "\n",
    "# A higher value of epsilon encourages more exploration, while a lower value of epsilon favors exploitation.\n",
    "epsilon = 0.9 # For epsilon-gready policy, Positive real number (0 < epsilon < 1)\n",
    "\n",
    "max_steps = 100 # Maximum number of steps that agent can take in environment\n",
    "\n",
    "lr_rate = 0.81 # Learning Rate of convergence to global minimum\n",
    "\n",
    "# A high discount factor means that future rewards are highly valued, while a \n",
    "# low discount factor means that immediate rewards are given greater weight\n",
    "gamma = 0.96 # Discount Factor, Positive real number (0 < gamma < 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating environment using gym package.\n",
    "# Default parameters:- gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=True)\n",
    "# For more information:- https://www.gymlibrary.dev/environments/toy_text/frozen_lake/\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\") # Using FrozenLake-v1. Since FrozenLake-v0 is depricated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observation states:-  16\n",
      "Number of action space :-  4\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of observation states:- \", env.observation_space.n)\n",
    "print(\"Number of action space :- \", env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing the Q-table with zeros for 2-D array of (observation_space, action_space).\n",
    "\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state):\n",
    "    \"\"\" \n",
    "    The choose_action() function used a Epsilon-Gready policy for Exploration and Exploitation.\n",
    "    \n",
    "    Exploration is done when uniform random number from 0 to 1 is less than epsilon value.\n",
    "    Else, Maximum value of the state and action pair is taken which is Exploitation.\n",
    "\n",
    "    Args:\n",
    "        state (int): Gets the current state as parameter/\n",
    "\n",
    "    Returns:\n",
    "        int: Returns action to be taken in that state\n",
    "    \"\"\"\n",
    "    action=0\n",
    "    if np.random.uniform(0, 1) < epsilon: # Epsilon-Gready policy\n",
    "        action = env.action_space.sample() # Exploration, Random action sample space\n",
    "    else:\n",
    "        action = np.argmax(Q[state, :]) # Exploitation, Maximum value is taken from (State, Action)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "Image(filename='Q-Formula.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(state, state2, reward, action):\n",
    "    \"\"\" \n",
    "    Updates the Q-table.\n",
    "    Agent learn to find a optimal policy by using bellman optimality equation.\n",
    "\n",
    "    Args:\n",
    "        state (int): Current state\n",
    "        state2 (int): Future state\n",
    "        reward (int): Reward if rached to goal state\n",
    "        action (int): action states\n",
    "    \"\"\"\n",
    "    predict = Q[state, action]\n",
    "    target = reward + gamma * np.max(Q[state2, :])\n",
    "    Q[state, action] = Q[state, action] + lr_rate * (target - predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with Reinforment Learning\n",
    "for episode in range(total_episodes):\n",
    "    state = env.reset() # Resetting the environment\n",
    "    t = 0\n",
    "    \n",
    "    while t < max_steps:\n",
    "        env.render() # Rendering the frozen-lake environment\n",
    "\n",
    "        action = choose_action(state)  # Taking action\n",
    "        # Returns new_state, reward after taking a action, done (boolean) whether reached to goal or not, information\n",
    "        state2, reward, done, info = env.step(action)  \n",
    "\n",
    "        learn(state, state2, reward, action) # Agent learns Q-table policy\n",
    "\n",
    "        state = state2 # Setting state to new state \n",
    "\n",
    "        t += 1\n",
    "       \n",
    "        if done: # If goal state is reached then true and loop breaks\n",
    "            break\n",
    "\n",
    "        time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumping the Q-table in pickle file for easy loading when needed.\n",
    "\n",
    "with open(\"frozenLake_qTable.pkl\", 'wb') as f:\n",
    "    pickle.dump(Q, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Table Representation\n",
    "\n",
    "# Q = [[0.61328675 0.58403178 0.59475622 0.60545697]\n",
    "#  [0.56521894 0.11293677 0.50370929 0.60326189]\n",
    "#  [0.59057488 0.58249108 0.57499097 0.59089953]\n",
    "#  [0.57255264 0.01738344 0.57441336 0.59935962]\n",
    "#  [0.62387633 0.5546865  0.71629256 0.11403143]\n",
    "#  [0.         0.         0.         0.        ]\n",
    "#  [0.64606929 0.71730524 0.73581135 0.11986364]\n",
    "#  [0.         0.         0.         0.        ]\n",
    "#  [0.61203901 0.70121817 0.69993882 0.79860727]\n",
    "#  [0.1464226  0.87045385 0.16544007 0.13804508]\n",
    "#  [0.91957935 0.91595476 0.57216828 0.68562543]\n",
    "#  [0.         0.         0.         0.        ]\n",
    "#  [0.         0.         0.         0.        ]\n",
    "#  [0.81536073 0.03436849 0.87831349 0.70812791]\n",
    "#  [0.95772482 0.88879209 0.92707794 0.99582979]\n",
    "#  [0.         0.         0.         0.        ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Episode:  0\n",
      "*** Episode:  1\n",
      "*** Episode:  2\n",
      "*** Episode:  3\n",
      "*** Episode:  4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "env = gym.make('FrozenLake-v1') # Create an instance of the FrozenLake-v1 environment\n",
    "\n",
    "# Un-pickling the pickle file using load() function. To convert from pickle to python object.\n",
    "with open(\"frozenLake_qTable.pkl\", 'rb') as f: \n",
    "\tQ = pickle.load(f)\n",
    "\n",
    "# Choosing the maximum value from the state and action\n",
    "def choose_action(state):\n",
    "\taction = np.argmax(Q[state, :])\n",
    "\treturn action\n",
    "\n",
    "# Rendering the environment and choosing the best action based on policy\n",
    "for episode in range(5):\n",
    "\n",
    "\tstate = env.reset()\n",
    "\tprint(\"*** Episode: \", episode)\n",
    "\tt = 0\n",
    "\twhile t < 100:\n",
    "\t\tenv.render() # Resetting the environment\n",
    "\n",
    "\t\taction = choose_action(state)  \n",
    "\t\t\n",
    "\t\tstate2, reward, done, info = env.step(action)  \n",
    "\t\t\n",
    "\t\tstate = state2\n",
    "\n",
    "\t\tif done:\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\ttime.sleep(0.5)\n",
    "\t\tos.system('clear')\n",
    "\n",
    "pygame.quit() # Closing the pygame window."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
